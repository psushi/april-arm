# Manual Dataset Collection V2 - Using Existing Project Infrastructure

This document describes the **V2 manual dataset collection system** that properly integrates with your existing project files (`aruco_detection.py`, `calibrate.py`, `camera_params.json`) instead of reimplementing everything from scratch.

## 🎯 Overview

The V2 manual collection system:
- **Uses existing project infrastructure** - integrates with your aruco_detection.py, calibrate.py, and camera_params.json
- **Supports ArUco cube detection** - works with cube faces (IDs 0-5) not single markers
- **Leverages existing calibration** - loads from calib.npz or camera_params.json
- **Inherits workspace bounds** - uses the same boundaries and visualization from main.py
- **Manual episode control** - dynamic start/stop recording with SPACE key
- **Real-time teleoperation** - toggle between mouse and ArUco cube control

## 📁 File Structure

### Core V2 Files
- **`collect_dataset_manual_v2.py`** - Main V2 collection script using existing infrastructure  
- **`MANUAL_COLLECTION_V2_README.md`** - This documentation

### Existing Project Files (Used by V2)
- **`aruco_detection.py`** - ArUco cube detection functions (`get_com_pose`, `detect_pose`)
- **`calibrate.py`** - Camera calibration generation script
- **`calib.npz`** - Camera calibration data (generated by calibrate.py)
- **`camera_params.json`** - Alternative calibration format
- **`main.py`** - Workspace boundaries, visualization, IK solver
- **`scene.xml`** / **`so_arm100.xml`** - Robot model files

## 🚀 Quick Start

### 1. Basic Manual Collection (Mouse Control)

```bash
# Run V2 manual collection
uv run python collect_dataset_manual_v2.py
```

**Controls:**
- **SPACE** - Start/Stop episode recording
- **Mouse** - Drag red target box to control arm
- **ESC** - Exit and save dataset

### 2. ArUco Cube Teleoperation Mode

```bash
# Ensure camera calibration exists (if not, run calibrate.py first)
ls calib.npz camera_params.json

# Run with ArUco cube integration
uv run python collect_dataset_manual_v2.py
```

**Controls:**
- **SPACE** - Start/Stop episode recording
- **A** - Toggle between ArUco cube and Manual control
- **H** - Show help
- **Move ArUco cube** - Control arm in ArUco mode

## 🔧 Infrastructure Integration

### ArUco Detection Integration

The V2 system properly uses your existing `aruco_detection.py`:

```python
# Uses existing functions from aruco_detection.py
from aruco_detection import detect_pose, get_com_pose, MARKER_SIZE

# Detects ArUco cube faces (IDs 0-5)
com_pos, com_rot = get_com_pose(corners, ids, camera_matrix, distortion_coeff)

# Uses proper cube center-of-mass calculation
marker_pos = com_pos[0][0]  # Get first detected cube position
```

### Camera Calibration Integration

Loads calibration from your existing files:

```python
# Priority order:
# 1. calib.npz (preferred - from calibrate.py)
# 2. camera_params.json (fallback)

if os.path.exists("calib.npz"):
    calib_data = np.load("calib.npz")
    camera_matrix = calib_data["mtx"]
    distortion_coeff = calib_data["dist"]
elif os.path.exists("camera_params.json"):
    with open("camera_params.json", 'r') as f:
        camera_params = json.load(f)
    camera_matrix = np.array(camera_params["camera_matrix"])
    distortion_coeff = np.array(camera_params["dist_coeffs"])
```

### Main.py Integration

Uses the same simulation structure and parameters:

```python
# Imports from main.py
from main import check_workspace_boundaries, WORKSPACE_BOUNDS, add_boundary_visualization

# Uses same simulation parameters
dt = 0.002  # From main.py
integration_dt = 1.0  # From main.py
damping = 1e-4  # From main.py

# Same gravity compensation setup
body_names = ["Rotation_Pitch", "Upper_Arm", "Lower_Arm", 
              "Wrist_Pitch_Roll", "Fixed_Jaw", "Moving_Jaw"]
model.body_gravcomp[body_ids] = 1.0

# Same inverse kinematics solver
jac = np.zeros((6, model.nv))
diag = damping * np.eye(6)
# ... same IK math as main.py
```

## 🎮 Detailed Controls

### Keyboard Controls

| Key | Action |
|-----|---------|
| `SPACE` | Start/Stop episode recording |
| `A` | Toggle ArUco cube/Manual control mode |
| `H` | Show help and control instructions |
| `ESC` | Exit application and save dataset |

### Control Modes

#### 1. Manual Mode (Default)
- Use mouse to drag the red target box in the MuJoCo viewer
- Arm follows the target using inverse kinematics from main.py
- Real-time workspace boundary enforcement

#### 2. ArUco Cube Mode (if calibration available)
- Move ArUco cube with faces labeled 0-5 in front of camera
- Uses existing `get_com_pose()` function for accurate center calculation
- Real-time coordinate transformation from camera to robot space
- Visual feedback with coordinate axes on each detected face

## 📊 Dataset Format (V2)

### Episode Structure
Each V2 episode contains enhanced metadata:

```python
episode_000000.hdf5:
├── observation/state: [N, 18]  # Joint states + end-effector pose
├── action: [N, 6]              # Joint commands
├── timestamp: [N]              # Time elapsed since episode start
└── attributes:
    ├── episode_id: 0
    ├── length: N
    ├── duration: X.X seconds
    ├── collection_mode: "manual_teleoperation_v2"
    ├── aruco_enabled: True/False
    └── uses_project_infrastructure: True
```

### Metadata (V2)
Enhanced metadata tracks infrastructure usage:

```json
{
  "codebase_version": "2.0.0",
  "collection_mode": "manual_teleoperation_v2",
  "uses_project_infrastructure": true,
  "calibration_source": "calib.npz",
  "aruco_enabled": true,
  "workspace_bounds": { "x_min": -0.2, "x_max": 0.2, ... }
}
```

## 🎥 ArUco Cube Detection (V2)

### Cube Setup
The V2 system works with ArUco cubes (not single markers):

```
ArUco Cube Faces:
┌─────────────────┐
│  ID 0 (top)     │
├─────────────────┤
│  ID 1 (bottom)  │  
├─────────────────┤
│  ID 2 (front)   │
├─────────────────┤
│  ID 3 (back)    │
├─────────────────┤
│  ID 4 (left)    │
├─────────────────┤
│  ID 5 (right)   │
└─────────────────┘
```

### Detection Process
Uses existing `aruco_detection.py` functions:

1. **Detect markers** on cube faces (IDs 0-5)
2. **Calculate center of mass** using `get_com_pose()`
3. **Apply coordinate transformation** to robot workspace
4. **Enforce workspace boundaries** using main.py functions
5. **Update target position** for inverse kinematics

### Coordinate Transformation
```python
# From aruco_detection.py coordinate system to robot workspace
robot_x = marker_pos[0]              # Left/right
robot_y = -marker_pos[2] + 0.5       # Forward/back (inverted Z)
robot_z = -marker_pos[1] + 0.3       # Up/down (inverted Y)

# Apply workspace constraints
aruco_pose = check_workspace_boundaries(aruco_pose)
```

## 🛡️ Workspace Safety (Inherited from main.py)

### Boundary Enforcement
Uses the same workspace bounds from main.py:

```python
WORKSPACE_BOUNDS = {
    'x_min': -0.2, 'x_max': 0.2,    # ±20cm lateral
    'y_min': -0.2, 'y_max': 0.2,    # ±20cm forward/back
    'z_min': 0.05, 'z_max': 0.6     # 5cm-60cm height
}
```

### Visual Feedback
- **Red wireframe box** shows workspace boundaries (from main.py)
- **Automatic position clamping** to safe limits
- **Warning messages** when targets exceed bounds

## 📈 Usage Workflow

### Setting Up Camera Calibration

1. **Capture calibration images** (if not done already):
   ```bash
   # Place chessboard images in photos/ folder
   mkdir -p photos
   # Take photos of chessboard pattern from different angles
   ```

2. **Run calibration** (if calib.npz doesn't exist):
   ```bash
   uv run python calibrate.py
   # This generates calib.npz
   ```

3. **Verify calibration files**:
   ```bash
   ls calib.npz camera_params.json
   ```

### Collecting Dataset

1. **Start collection**:
   ```bash
   uv run python collect_dataset_manual_v2.py
   ```

2. **Choose control mode**:
   - Press `A` to toggle between Manual/ArUco control
   - Manual: Use mouse to control target
   - ArUco: Move cube in front of camera

3. **Record episodes**:
   ```
   Press SPACE → Start recording
   Demonstrate task → Teleoperate the arm  
   Press SPACE → Stop and save episode
   Repeat → Collect multiple demonstrations
   ```

4. **Exit and inspect**:
   ```bash
   Press ESC → Exit collection
   
   # Inspect collected data
   uv run python inspect_dataset.py ./datasets/so100_manual_dataset_v2
   ```

### Example Session Output

```
🚀 Starting Manual Dataset Collection V2 with Project Infrastructure
✅ Loaded camera calibration from calib.npz
✅ Loaded camera parameters from camera_params.json
🎥 ArUco detection thread started - Press 'q' in ArUco window to stop

✅ Ready for data collection!
📍 Press SPACE to start recording your first episode

🎬 Started recording episode 0
🔴 Recording Episode 0: 150 steps, 5.2s | Mode: ArUco
💾 Saved episode 0: 150 steps, 5.20s duration
✅ Episode 0 saved successfully

📊 Final Dataset Summary:
   Total episodes collected: 3
   Dataset location: datasets/so100_manual_dataset_v2
   ArUco detection: Enabled
   Infrastructure: Using existing project files
   Calibration: calib.npz
✅ Dataset ready for use with LeRobot!
```

## 🔧 Customization

### Modify ArUco Coordinate Transformation

Edit the transformation in `collect_dataset_manual_v2.py`:

```python
# Custom coordinate mapping for your specific camera setup
robot_x = marker_pos[0] * x_scale + x_offset
robot_y = -marker_pos[2] * y_scale + y_offset  
robot_z = -marker_pos[1] * z_scale + z_offset
```

### Adjust Workspace Bounds

Modify in `main.py` (affects both main simulation and V2 collection):

```python
 WORKSPACE_BOUNDS = {
     'x_min': -0.3, 'x_max': 0.3,  # Larger workspace
     'y_min': -0.3, 'y_max': 0.3,
     'z_min': 0.1,  'z_max': 0.8
 }
```

### Camera Calibration Parameters

Regenerate calibration for better accuracy:

```bash
# Take more calibration photos
# Put them in photos/ folder
uv run python calibrate.py

# Or manually edit camera_params.json for fine-tuning
```

## 🐛 Troubleshooting

### ArUco Detection Issues

**Problem**: No ArUco cube detected
```bash
# Check camera access
ls /dev/video*

# Verify calibration files exist
ls calib.npz camera_params.json

# Test standalone ArUco detection
uv run python aruco_detection.py calib.npz
```

**Problem**: Poor cube tracking
- Ensure cube has clear markers with IDs 0-5
- Use good lighting conditions
- Keep cube faces visible to camera
- Check that cube dimensions match `aruco_detection.py` settings

### Calibration Issues

**Problem**: Camera calibration failed
```bash
# Check if photos exist
ls photos/*.jpg

# Ensure chessboard pattern is correct (8x6 as defined in calibrate.py)
# Retake photos with better lighting/angles
```

**Problem**: ArUco coordinates are wrong
- Adjust coordinate transformation in V2 script
- Check camera orientation relative to robot
- Verify cube is correct size (matches MARKER_SIZE in aruco_detection.py)

### Integration Issues

**Problem**: Import errors from existing files
```bash
# Ensure all files are in same directory
ls main.py aruco_detection.py calibrate.py

# Check for syntax errors in existing files
uv run python -c "from main import check_workspace_boundaries"
uv run python -c "from aruco_detection import get_com_pose"
```

## 📝 V1 vs V2 Comparison

| Feature | V1 (From Scratch) | V2 (Using Project Infrastructure) |
|---------|-------------------|-----------------------------------|
| ArUco Detection | Custom implementation | Uses existing `aruco_detection.py` |
| Calibration | Custom calib.npz loading | Uses `calib.npz` + `camera_params.json` |
| Marker Type | Single ArUco markers | ArUco cube with faces 0-5 |
| Workspace Bounds | Hardcoded copy | Imports from `main.py` |
| Coordinate Transform | Basic transformation | Proper center-of-mass calculation |
| Simulation Setup | Custom parameters | Inherits from `main.py` |
| Code Reuse | Reimplemented everything | Leverages existing codebase |

## 🎯 Next Steps

1. **Use V2 for data collection** - Better integration with your existing workflow
2. **Collect ArUco cube demonstrations** - More accurate pose detection than single markers  
3. **Train policies** - Use collected data with LeRobot
4. **Extend infrastructure** - Add more features to existing aruco_detection.py
5. **Real robot deployment** - Adapt V2 system for real SO-100 hardware

## 📄 Files Summary

- **`collect_dataset_manual_v2.py`** (598 lines) - V2 collection using project infrastructure
- **`MANUAL_COLLECTION_V2_README.md`** - This documentation

The V2 system provides better integration with your existing codebase while maintaining all the manual control features for high-quality teleoperation data collection.